{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b43613e7",
   "metadata": {},
   "source": [
    "# GP Tutorial-E02: Simple GP Implementation in an hour\n",
    "Author: Wei W. Xing (wxing.me) <br />\n",
    "Email: wayne.xingle@gmail.com   <br />\n",
    "Date: 2022-03-23    <br />\n",
    "Acknowledgement: Many thanks to my student Yu Xing to help to put this tutorial together.   <br />\n",
    "\n",
    "Abstract: Despite that Gaussian process (GP) has been a long-existing and common used technique, which comes with many useful tutorials, books, articles, and blogs, I found that many (including me as a young student) struggled to put GP into real projects. I believe that the easiest way to learn something is by doing it, instead of preparing all the necessary mathematical backgrounds and diving into theory. Thus, I wrote this tutorials, trying to make it straight-forward to utilize the fancy GP technique, particularly for engineering students.Hopes that you find the tutorial useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ed342",
   "metadata": {},
   "source": [
    "In this tutorial, we consider a simple case of single output problem, i.e., $y \\in \\mathcal{R}^1$ is a scalar value, corresponding to $\\mathcal{x} \\in \\mathcal{R}^D$, a $D$-value input vector variable.\n",
    "<!-- For the tutorial purpose, we do not  -->\n",
    "<!-- 为了能够降低阅读的难度，我们在Tutorial-E01中将介绍最为简单的GP的代码，也就是指输出$y \\in \\mathcal{R}^1$为1维标量的情况。 -->\n",
    "<!-- 为了能够更加方便阅读以及理解，我们将GP整体的类定义拆分成了相互独立的函数.在最后会将全部包括定义成class的代码展示 -->\n",
    "\n",
    "Let's get started. This implementation is based on Torch, so let's start from importing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "50c77362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "print(torch.__version__)\n",
    "# I use torch (1.11.0) for this work. lower version may not work.\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True' # Fixing strange error if run in MacOS \n",
    "\n",
    "JITTER = 1e-6\n",
    "EPS = 1e-10\n",
    "PI = 3.1415"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d3d8c",
   "metadata": {},
   "source": [
    "#### 1. Generating synthetic data\n",
    "<!-- 假设真实函数为$y = (6x-2)^2sin(12x-4)$。训练数据 $xtr, ytr$ 分别是$(32, 1)$的列向量，测试数据集$xte, yte$则是选择了100个点，是$(100, 1)$的列向量。 -->\n",
    "Consider the following function as our target function.\n",
    "$$y = (6x-2)^2sin(12x-4)$$\n",
    "We will generate training set $xtr, ytr$ containing 32 samples and testing set $xte, yte$ 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7692b6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtr.size: torch.Size([32, 1]) ytr.size: torch.Size([32, 1])\n",
      "xte.size: torch.Size([100, 1]) yte.size: torch.Size([100, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_set\n",
    "xtr = torch.rand(32, 1)\n",
    "ytr = ((6*xtr - 2)**2) * torch.sin(12*xtr - 4) + torch.randn(32, 1) * 1\n",
    "\n",
    "#test_set\n",
    "xte = torch.linspace(0, 1, 100).view(-1,1)\n",
    "yte = ((6*xte - 2)**2) * torch.sin(12*xte - 4)\n",
    "\n",
    "#plot the data\n",
    "print(\"xtr.size:\", xtr.size(), \"ytr.size:\", ytr.size())\n",
    "print(\"xte.size:\", xte.size(), \"yte.size:\", yte.size())\n",
    "plt.plot(xtr.numpy(), ytr.numpy(), 'b+')\n",
    "plt.plot(xte.numpy(), yte.numpy(), 'r-', alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32aeed6",
   "metadata": {},
   "source": [
    "<!-- #### 2. 设定可优化参数以及核函数 -->\n",
    "#### 2. Kernel functions and model parameter (hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a97c038",
   "metadata": {},
   "source": [
    "There are two functions, the mean function and the kernel function, that we need to optimize.\n",
    "We cannot optimize a \"function\" easily. Thus we give these functions a particular form to optimize their parameters.\n",
    "In this tutorial, we specify the mean function as a zero function (and there are nothing to optimize for it.)\n",
    "For the kernel function, we use the most commonly used automatic relevance determinant (ARD) kernel:\n",
    "$$k_{ard}(\\mathbf{x}, \\mathbf{x}') = a \\cdot \\exp(-\\frac{1}{2} \\sum_{d=1}^D{ (\\frac{x_d-x'_d}{l_d}})^2 )$$\n",
    "where $a$ is the amplitude of the kernel function, and $l_d$ is called length scale for d-dimensional input ${x}_d$. We can see that $\\mathbf{l}$ controls the contribution of each input dimension, e.g., a larger $l_d$ means that $x_d$ has a larger contribution (compared to other dimensions) on the system and vise versa. Thus this kernel has the name \"ARD\".\n",
    "\n",
    "One more thing before we start coding. Note that $l_d$ and $a$ are both positive parameters. If we directly define them as torch parameters, they might be pushed to negative during the optimization. Thus we need to make sure that they are positive. One simple way to ensure that is to define them as their logarithmic values and take their exponential values when we use them.\n",
    "\n",
    "Now let's define the kernel function, which returns the kernel matrix $K$ for two sets for input data.\n",
    "\n",
    "\n",
    "<!-- 高斯过程中，需要优化的变量是GP的均值函数（中的参数）以及核函数（中的参数）。\n",
    "在这个例子中我们假设均值函数为0，因此不需要优化他。\n",
    "<!-- 在这个最简单的情况下，我们已经设定均值函数为0，所以只需要优化协方差函数，也就是这个模型的kernel，因此在下面的cell里里设定的需要优化的参数其实就是kernel中的超参数。 -->\n",
    "<!-- 我们采用最为经典的 automatic relevance determinant （ARD）核函数： -->\n",
    "<!-- 这里$l_d$对输入的每个维度进行一个缩放，把那些对系统影响最大的因素（对应着$l_d$很大）筛选出来，所以叫做ARD；$a$ 是kernel的scale。 -->\n",
    "<!-- 计算kernel matrix的时候我们通常传入的是多个$\\mathbf{x}$, -->\n",
    "<!-- 另外，由于$l_d$ 和 $a$ 必须大于0。因此我们在写程序的时候定义的是 $\\log(l_d)$。当需要使用的时候再对其进行指数运算，这样保证我们的参数$\\exp{(\\log(l_d))}$永远是正的。 -->\n",
    "\n",
    "<!-- 核函数返回是两个或两组输入之间的核矩阵，我们可以简单地定义核函数： --> \n",
    "\n",
    "\n",
    "<!-- 下面的cell中我们定义了核函数，具体的核函数表达式为(SE/OU)：$$k_{ard} = a\\cdot exp(-\\frac{(x-x')^2-x^2-x'^2}{a})$$ 上述的公式仅是点对点的kernel的值，为了更好的处理大量的数据我们定义的kernel函数将返回一个大小为$(n_X,n_{X2})$的矩阵，其中的第i行，j列元素的含义是：$K_{ij} = k(X_i,X2_j)$。 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2190321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel parameters\n",
    "log_length_scale = nn.Parameter(torch.zeros(xte.size(1)))\n",
    "log_scale = nn.Parameter(torch.zeros(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "019a10f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8437, 0.8688,  ..., 0.9118, 0.9089, 0.8885],\n",
      "        [0.8437, 1.0000, 0.9986,  ..., 0.9883, 0.9894, 0.9953],\n",
      "        [0.8688, 0.9986, 1.0000,  ..., 0.9949, 0.9956, 0.9990],\n",
      "        ...,\n",
      "        [0.9118, 0.9883, 0.9949,  ..., 1.0000, 1.0000, 0.9984],\n",
      "        [0.9089, 0.9894, 0.9956,  ..., 1.0000, 1.0000, 0.9988],\n",
      "        [0.8885, 0.9953, 0.9990,  ..., 0.9984, 0.9988, 1.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def kernel(X1, X2, log_length_scale, log_scale): \n",
    "    length_scale = torch.exp(log_length_scale)\n",
    "    K = torch.zeros(X1.size(0), X2.size(0))\n",
    "    \n",
    "    for i in range(X1.size(0)):\n",
    "        for j in range(X2.size(0)):\n",
    "            for d in range(X1.size(1)):\n",
    "                K[i,j] = torch.exp(-0.5 * ((X1[i,d] - X2[j,d])**2 / length_scale[d]**2).sum() ) * torch.exp(log_scale)\n",
    "    return log_scale.exp() * K\n",
    "\n",
    "K1 = kernel(xtr, xtr, log_length_scale, log_scale)\n",
    "print(K1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb9268",
   "metadata": {},
   "source": [
    "*However, we should *NEVER** use a for loop unless we have to. \n",
    "Using the for loop will take a lot time and memory we should have not paid.\n",
    "Let's redo the kernel function in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c88c4cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8437, 0.8688,  ..., 0.9118, 0.9089, 0.8885],\n",
      "        [0.8437, 1.0000, 0.9986,  ..., 0.9883, 0.9894, 0.9953],\n",
      "        [0.8688, 0.9986, 1.0000,  ..., 0.9949, 0.9956, 0.9990],\n",
      "        ...,\n",
      "        [0.9118, 0.9883, 0.9949,  ..., 1.0000, 1.0000, 0.9984],\n",
      "        [0.9089, 0.9894, 0.9956,  ..., 1.0000, 1.0000, 0.9988],\n",
      "        [0.8885, 0.9953, 0.9990,  ..., 0.9984, 0.9988, 1.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor(8.1508e-07, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "def kernel(X1, X2, log_length_scale, log_scale): # 定义核函数没有加linear\n",
    "\n",
    "    X1 = X1 / log_length_scale.exp()**2\n",
    "    X2 = X2 / log_length_scale.exp()**2\n",
    "\n",
    "    X1_norm2 = X1 * X1\n",
    "    X2_norm2 = X2 * X2\n",
    "\n",
    "    K = -2.0 * X1 @ X2.t() + X1_norm2.expand(X1.size(0), X2.size(0)) + X2_norm2.t().expand(X1.size(0), X2.size(0))  #this is the effective Euclidean distance matrix between X1 and X2.\n",
    "    K = log_scale.exp() * torch.exp(-0.5 * K)\n",
    "    return K\n",
    "\n",
    "K2 = kernel(xtr, xtr, log_length_scale, log_scale)\n",
    "print(K2)\n",
    "print( (K1 - K2).norm() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa4bed",
   "metadata": {},
   "source": [
    "We can see that the resulting kernel is the same, whereas the execution time is much faster. You can test a larger xtr to see the difference.\n",
    "\n",
    "We also need to define the noise $\\sigma^2$ of the GP.\n",
    "To be easier to introduce conjugate prior in a later tutorial, instead of defining $\\sigma^2$, we define its inverse $\\beta=1/\\sigma^2$. Again, we need the value positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "34dba8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_beta = nn.Parameter(torch.ones(1) * -4) # this is a large noise. we optimize to shrink it to a proper value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c072b",
   "metadata": {},
   "source": [
    "<!-- #### 3. 定义loss损失函数以及优化器 -->\n",
    "#### 3. Negative log likelihood (nll) \n",
    "For almost all regression problems, we aim to minimize a defined loss function. In the GP case, the loss function is the negative log likelihood.\n",
    "It is negative because we want to minimize as like what we do for a loss function; we take the logarithm of the likelihood because it gives a clear form, and the logarithm does not change the monotonicity of the function.\n",
    "The log likelihood is defined as:\n",
    "$$L=-\\frac{1}{2}\\mathbf{y}^T (\\mathbf{K}+\\sigma^2 \\mathbf{I})^{-1}\\mathbf{y}-\\frac{1}{2}\\log(|\\mathbf{K}+\\sigma^2 \\mathbf{I}|)-\\frac{n}{2}log(2\\pi)$$\n",
    "where $\\mathbf{y}$ is the observed data arranged as a $N\\times1$ vector, $\\mathbf{K}$ is the $N\\times N$ kernel matrix (of the training data), $\\sigma^2$ is the noise variance, and $\\mathbf{I}$ is the identity matrix.\n",
    "Rewrite our loss function in nll, we have\n",
    "$$nll=\\frac{1}{2}\\mathbf{y}^T\\mathbf{\\Sigma}^{-1}\\mathbf{y}+\\frac{1}{2}\\log(|\\mathbf{\\Sigma}|)+\\frac{n}{2}log(2\\pi)$$\n",
    "where we define $\\mathbf{\\Sigma}=\\mathbf{K}+\\sigma^2 \\mathbf{I}$ as the inverse of the kernel matrix.\n",
    "\n",
    "We can directly solve code up the nll function and call it in the optimizer. However, we should NOT inverse a matrix unless we have to.\n",
    "To calculate the nll in a more efficeint way and stable, we can use the following trick:\n",
    "\n",
    "First compute the cholesky decomposition of the kernel matrix, $\\mathbf{\\Sigma} = \\mathbf{L} \\mathbf{L}^T$.\n",
    "For the first term in the nll, we have,\n",
    "$$ \\mathbf{y}^T (\\mathbf{K}+\\sigma^2 \\mathbf{I})^{-1}\\mathbf{y} = \\mathbf{y}^T (\\mathbf{L} \\mathbf{L}^T)^{-1} \\mathbf{y} = \\mathbf{y}^T \\mathbf{L}^{-T} \\mathbf{L}^{-1} \\mathbf{y}  = (\\mathbf{L}^{-1} \\mathbf{y})^T \\mathbf{L}^{-1} \\mathbf{y}  $$\n",
    "The formula allows us to avoid inverting the kernel matrix by solving a linear system, $\\mathbf{L} \\mathbf{\\gamma} = \\mathbf{y}$, which gives us $\\mathbf{L}^{-1} \\mathbf{y} = \\mathbf{\\gamma}$.\n",
    "Once we obtain $\\mathbf{\\gamma}$, we can calculate the first term in nll as the L2 norm of  $\\mathbf{\\gamma}$.\n",
    "\n",
    "<!-- Remember that we should not invert a matrix unless we have to.  -->\n",
    "<!-- Define , which will becomes handy lately. -->\n",
    "For the second term in the nll, we have,\n",
    "$$ \\log(|\\mathbf{\\Sigma}|) = \\log(| \\mathbf{L} \\mathbf{L}^T |) =  \\log(|\\mathbf{L}| |\\mathbf{L}^T|) = \\log(\\prod_{i=1}^{N} L_{ii} \\prod_{i=1}^{N} L_{ii}) = 2\\sum_{i=1}^{N}\\log(L_{ii})$$\n",
    "\n",
    "With these two process, the nll becomes:\n",
    "$$nll=\\frac{1}{2} \\mathbf{\\gamma}^T \\mathbf{\\gamma} + \\sum_{i=1}^{N}\\log(L_{ii}) + \\frac{n}{2}log(2\\pi)$$\n",
    "\n",
    "Let's code these up now. It will turn out simple.\n",
    "\n",
    "<!-- 在高斯过程中，我们是通过找极大似然函数对GP进行优化，那么在这里我们的loss就应该是这个模型的似然函数（likelihood）。为了能够更方便计算，通常会对似然函数取log，且为了方便torch中的优化器调用，可以利用求极小值的优化器来对log_likelihood的相反数进行优化。因此下面cell中的negative_log_likelihood函数是模型的-log(likelihood)。\n",
    "$$-L=\\frac{1}{2}y^T\\Sigma^{-1}y+\\frac{1}{2}log(|\\Sigma|)+\\frac{n}{2}log(2\\pi)$$\n",
    "其中$\\Sigma = K(X,X)+ \\frac{1}{\\beta} I+ Jitter\\cdot I$，是加上噪声影响后的协方差矩阵。\n",
    "\n",
    "但是，由于对一个矩阵求逆计算复杂且耗时较长，因此我们采用cholesky分解，将协方差矩阵$\\Sigma$分解为下三角矩阵$L$和其转置矩阵的乘积，也就是：$\\Sigma = LL^T$，因此我们可以先计算中间变量$\\alpha = L^{-1}Y$，降低计算的复杂度，具体推导如下：\n",
    "\\begin{aligned}\n",
    "-L=&\\frac{1}{2}y^T\\Sigma^{-1}y+\\frac{1}{2}log(|\\Sigma|)+\\frac{n}{2}log(2\\pi)\\\\\n",
    "=&\\frac{1}{2}y^T(LL^T)^{-1}y+\\frac{1}{2}log(|LL^T|)+\\frac{n}{2}log(2\\pi)\\\\\n",
    "=&\\frac{1}{2}(yL^{-1})^TyL^{-1}+\\frac{1}{2}log(|LL^T|)+\\frac{n}{2}log(2\\pi)\\\\\n",
    "=&\\alpha^T\\alpha+\\frac{1}{2}log(|\\prod_{i=1}^nL_{ii}|)+\\frac{n}{2}log(2\\pi)\\\\\n",
    "=&\\sum_{i=1}^{n}\\alpha_i^2+\\sum_{i=1}^{n}L_{ii}+\\frac{n}{2}log(2\\pi)\n",
    "\\end{aligned}\n",
    "在计算$\\Sigma$矩阵行列式时利用cholesky分解的特点，有结论：$|\\Sigma|=|\\prod_{i=1}^nL_{ii}|$，返回值为标量。 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f3b4cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(X, Y, log_length_scale, log_scale, log_beta):\n",
    "    y_num = Y.size(0)\n",
    "    Sigma = kernel(X, X, log_length_scale, log_scale) + log_beta.exp().pow(-1) * torch.eye(X.size(0)) + JITTER * torch.eye(X.size(0))   # add JITTER here to avoid singularity\n",
    "    \n",
    "    L = torch.linalg.cholesky(Sigma)\n",
    "    #option 1 (use this if torch supports)\n",
    "    gamma,_ = torch.triangular_solve(Y, L, upper = False)\n",
    "    #option 2\n",
    "    # gamma = L.inverse() @ Y       # we can use this as an alternative because L is a lower triangular matrix.\n",
    "    \n",
    "    nll =  0.5 * (gamma ** 2).sum() +  L.diag().log().sum()  + 0.5 * y_num * torch.log(2 * torch.tensor(PI))\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218da11",
   "metadata": {},
   "source": [
    "<!-- 当设定好loss函数以后就可以借用torch.optim中的优化器对模型进行优化，求极小值。 -->\n",
    "#### 4. Calling the optimizer\n",
    "With the loss/nll defined, we can simply call an optimizer to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "28a06a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 nnl:102.58795\n",
      "iter 1 nnl:102.51726\n",
      "iter 2 nnl:102.44745\n",
      "iter 3 nnl:102.37856\n",
      "iter 4 nnl:102.31059\n"
     ]
    }
   ],
   "source": [
    "def train_adam(X, Y, log_length_scale, log_scale, log_beta, niter = 50, lr = 0.001):\n",
    "    optimizer = torch.optim.Adam([log_beta, log_length_scale, log_scale], lr = lr)\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(niter):\n",
    "        optimizer.zero_grad()\n",
    "        # self.update()\n",
    "        loss = negative_log_likelihood(X, Y, log_length_scale, log_scale, log_beta)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print the nll\n",
    "        # print('iter', i, ' nnl:', loss.item())\n",
    "        print('iter', i, 'nnl:{:.5f}'.format(loss.item()))\n",
    "        # print the likelihood\n",
    "        # print('iter', i, 'nnl:{:.5f}'.format(loss.item()),'likelihood:{:.9f}'.format((-loss).exp().item()) )\n",
    "        \n",
    "train_adam(xtr, ytr,log_length_scale, log_scale, log_beta, 5, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd16b4d",
   "metadata": {},
   "source": [
    "<!-- #### 4. 预测 Predictive Prosterior  -->\n",
    "#### 5. Predictive posterior \n",
    "\n",
    "With the hyperparameters optimized, we can now calculate the predictive posterior following the standard formulations,\n",
    "\\begin{aligned}\n",
    "&\\mu=\\mathbf{k}^* \\Sigma^{-1} \\mathbf{y}\\\\\n",
    "&s^2=\\mathbf{k}_{**}- (\\mathbf{k}^*)^T \\mathbf{\\Sigma}^{-1} \\mathbf{k}^* + {1}/{\\beta}\\\\\n",
    "\\end{aligned}\n",
    "As we mentioned, we should not invert a matrix unless we have to.\n",
    "Again, we utilize the cholesky decomposition to obtain the predictive posterior.\n",
    "Let define $\\mathbf{\\Sigma}^{-1} \\mathbf{y} =  \\mathbf{\\alpha}$, which is conventionally used in the literature and open source codes. \n",
    "The advantage of introducing $\\mathbf{\\alpha}$ is that 1) we can use the cholesky decomposition to compute the predictive posterior; 2) it saves storage and provides quick prediction of the posterior (no more inversion required).\n",
    "<!-- Using $\\mathbf{\\gamma}$,  -->\n",
    "Using $\\mathbf{L}$ to get $\\mathbf{\\alpha}$, we solve the $\\mathbf{L} \\mathbf{\\gamma} = \\mathbf{y}$ first and then compute $\\mathbf{L}^T \\mathbf{y} = \\mathbf{\\gamma}$, which, as you might have seen, is sometimes written in a compact way\n",
    "$$\\mathbf{\\alpha} = \\mathbf{L}^T \\backslash \\mathbf{L} \\backslash \\mathbf{y}$$\n",
    "\n",
    "The computation of $(\\mathbf{k}^*)^T \\mathbf{\\Sigma}^{-1} \\mathbf{k}^*$ is similar to the computation of $\\mathbf{\\gamma}^T \\mathbf{\\gamma}$ in the previous section.\n",
    "\n",
    "<!-- 在预测模块，需要预测的主要为预测点的均值与方差，值得注意的是虽然模型假设均值函数为0但是预测点的均值不必须为0由预测值来决定。预测点的均值与方差表达式如下：\n",
    "\\begin{aligned}\n",
    "&\\mu=k(x^*,x)\\Sigma^{-1}Y=k(x^*,x)(LL^T)^{-1}Y\\\\\n",
    "&s^2=k_{**}-k(x,x^*)\\Sigma^{-1}k(x^*,x)\n",
    "\\end{aligned}\n",
    "为了方便计算，我们依旧利用cholesky分解，$\\Sigma=LL^T$，并且在方差部分由于函数设置是可以多个预测点同时进行计算，所以方差部分，我们仅需要预测方差矩阵的对角线部分的数值。因为噪声较小，所以课假设噪声不影响求逆。$$diag(s^2)=a\\cdot I-diag(k(x,x^*)(LL^T)^{-1}k(x^*,x))+\\frac{1}{\\beta}\\cdot I$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c2a7a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, Xte, log_length_scale, log_scale, log_beta, Y):\n",
    "    n_test = Xte.size(0)\n",
    "    Sigma = kernel(X, X, log_length_scale, log_scale) + log_beta.exp().pow(-1) * torch.eye(\n",
    "        X.size(0)) + JITTER * torch.eye(X.size(0))\n",
    "    kx = kernel(X, Xte, log_length_scale, log_scale)\n",
    "    L = torch.cholesky(Sigma)\n",
    "    \n",
    "    # option 1\n",
    "    mean = kx.t() @ torch.cholesky_solve(Y, L)  # torch.linalg.cholesky()\n",
    "    # option 2\n",
    "    # mean = kx @ torch.L.t().inverse() @ L.inverse() @ Y\n",
    "    \n",
    "    # LinvKx = L.inverse() @ kx.t()  # TODO: the inverse for L should be cheap. check this.\n",
    "        # torch.cholesky_solve(kx.t(), L)\n",
    "    LinvKx,_ = torch.triangular_solve(kx, L, upper = False)\n",
    "    # option 1, standard way\n",
    "    # var_diag = kernel(Xte, Xte, log_length_scale, log_scale).diag().view(-1,1) - (LinvKx.t() @ LinvKx).diag().view(-1, 1)\n",
    "    # option 2, a faster way\n",
    "    var_diag = log_scale.exp().expand(n_test, 1) - (LinvKx**2).sum(dim = 0).view(-1, 1)\n",
    "    \n",
    "    var_diag = var_diag + log_beta.exp().pow(-1)\n",
    "    return mean, var_diag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b00a1",
   "metadata": {},
   "source": [
    "#### 6. Testing and validating\n",
    "let try our GP model on the synthetic data and see how well it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5c1078f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 nnl:102.24358\n",
      "iter 1 nnl:101.62195\n",
      "iter 2 nnl:101.10047\n",
      "iter 3 nnl:100.68681\n",
      "iter 4 nnl:100.38427\n",
      "iter 5 nnl:100.18786\n",
      "iter 6 nnl:100.07895\n",
      "iter 7 nnl:100.01997\n",
      "iter 8 nnl:99.95547\n",
      "iter 9 nnl:99.83573\n",
      "iter 10 nnl:99.65105\n",
      "iter 11 nnl:99.43856\n",
      "iter 12 nnl:99.25546\n",
      "iter 13 nnl:99.13422\n",
      "iter 14 nnl:99.05937\n",
      "iter 15 nnl:98.99283\n",
      "iter 16 nnl:98.90590\n",
      "iter 17 nnl:98.78535\n",
      "iter 18 nnl:98.62592\n",
      "iter 19 nnl:98.42409\n",
      "iter 20 nnl:98.17632\n",
      "iter 21 nnl:97.88081\n",
      "iter 22 nnl:97.54082\n",
      "iter 23 nnl:97.16743\n",
      "iter 24 nnl:96.77485\n",
      "iter 25 nnl:96.35686\n",
      "iter 26 nnl:95.85793\n",
      "iter 27 nnl:95.20300\n",
      "iter 28 nnl:94.36958\n",
      "iter 29 nnl:93.39724\n",
      "iter 30 nnl:92.33742\n",
      "iter 31 nnl:91.19550\n",
      "iter 32 nnl:89.92153\n",
      "iter 33 nnl:88.46913\n",
      "iter 34 nnl:86.84787\n",
      "iter 35 nnl:85.11759\n",
      "iter 36 nnl:83.35136\n",
      "iter 37 nnl:81.59217\n",
      "iter 38 nnl:79.83046\n",
      "iter 39 nnl:78.05396\n",
      "iter 40 nnl:76.30730\n",
      "iter 41 nnl:74.64482\n",
      "iter 42 nnl:73.06537\n",
      "iter 43 nnl:71.53526\n",
      "iter 44 nnl:70.05692\n",
      "iter 45 nnl:68.67963\n",
      "iter 46 nnl:67.43968\n",
      "iter 47 nnl:66.30534\n",
      "iter 48 nnl:65.25889\n",
      "iter 49 nnl:64.36870\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_adam(xtr, ytr, log_length_scale, log_scale, log_beta, 50, 0.1)\n",
    "with torch.no_grad():\n",
    "    ypred, yvar = forward(xtr, xte, log_length_scale, log_scale, log_beta, ytr)\n",
    "    \n",
    "plt.errorbar(xte.numpy().reshape(100), ypred.detach().numpy().reshape(100),\n",
    "             yerr=yvar.sqrt().squeeze().detach().numpy(), fmt='r-.', alpha=0.2)\n",
    "plt.plot(xtr.numpy(), ytr.numpy(), 'b+')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
