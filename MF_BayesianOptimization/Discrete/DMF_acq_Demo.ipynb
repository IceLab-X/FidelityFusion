{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.path.realpath(\".\")) + '/FidelityFusion')\n",
    "\n",
    "import GaussianProcess.kernel as kernel\n",
    "from FidelityFusion_Models import *\n",
    "from FidelityFusion_Models.MF_data import MultiFidelityDataManager\n",
    "from DMF_acq import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Simple Two fidelity objective function\n",
    "This is a very simple two fidelity sample, named as 'Non Linear Sin' function.\n",
    "\\begin{aligned}\n",
    "    f_{low}(x) &= \\sin(8\\pi x),\\\\\n",
    "    f_{high}(x) &= (x - \\sqrt{2})f_{low}(x)^2.\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "We use this objective function to generate a small size training data set, with 8 low-fidleity samples and 4 high-fidelity training samples under the non-subset assumption.\n",
    "\n",
    "To be mentioned, the fidelity indicator starts from 0, which means 0 represents low-fidleity and 1 stands for high-fidleity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x, s):\n",
    "    xtr = x\n",
    "    if s == 0:\n",
    "        Ytr = torch.sin(xtr * 8 * torch.pi)\n",
    "    else:\n",
    "        Ytr_l = torch.sin(xtr * 8 * torch.pi)\n",
    "        Ytr = (xtr - torch.sqrt(torch.ones(xtr.shape[0])*2).reshape(-1, 1)) * torch.pow(Ytr_l, 2)\n",
    "    \n",
    "    return Ytr\n",
    "\n",
    "train_xl = torch.rand(8, 1) * 10\n",
    "train_xh = torch.rand(4, 1) * 10\n",
    "train_yl = objective_function(train_xl, 0)\n",
    "train_yh = objective_function(train_xh, 1)\n",
    "\n",
    "data_shape = [train_yl[0].shape, train_yh[0].shape]\n",
    "\n",
    "initial_data = [\n",
    "                    {'fidelity_indicator': 0,'raw_fidelity_name': '0', 'X': train_xl, 'Y': train_yl},\n",
    "                    {'fidelity_indicator': 1, 'raw_fidelity_name': '1','X': train_xh, 'Y': train_yh},\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the surrogate model: ResGP\n",
    "\n",
    "These part we use the simplest surrogate model ResGP as our MF model and train by the initiated training dataset. It can also be replaced by other MF model in FidelityFusion Models (i.e. AR, GAR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fidelity: 0 iter 0 nll:10.55080\n",
      "fidelity: 0 iter 1 nll:10.54580\n",
      "fidelity: 0 iter 2 nll:10.54081\n",
      "fidelity: 0 iter 3 nll:10.53581\n",
      "fidelity: 0 iter 4 nll:10.53082\n",
      "fidelity: 0 iter 5 nll:10.52583\n",
      "fidelity: 0 iter 6 nll:10.52085\n",
      "fidelity: 0 iter 7 nll:10.51587\n",
      "fidelity: 0 iter 8 nll:10.51089\n",
      "fidelity: 0 iter 9 nll:10.50591\n",
      "fidelity: 0 iter 10 nll:10.50094\n",
      "fidelity: 0 iter 11 nll:10.49597\n",
      "fidelity: 0 iter 12 nll:10.49100\n",
      "fidelity: 0 iter 13 nll:10.48604\n",
      "fidelity: 0 iter 14 nll:10.48108\n",
      "fidelity: 0 iter 15 nll:10.47612\n",
      "fidelity: 0 iter 16 nll:10.47116\n",
      "fidelity: 0 iter 17 nll:10.46622\n",
      "fidelity: 0 iter 18 nll:10.46127\n",
      "fidelity: 0 iter 19 nll:10.45633\n",
      "fidelity: 0 iter 20 nll:10.45139\n",
      "fidelity: 0 iter 21 nll:10.44645\n",
      "fidelity: 0 iter 22 nll:10.44152\n",
      "fidelity: 0 iter 23 nll:10.43659\n",
      "fidelity: 0 iter 24 nll:10.43167\n",
      "fidelity: 0 iter 25 nll:10.42675\n",
      "fidelity: 0 iter 26 nll:10.42184\n",
      "fidelity: 0 iter 27 nll:10.41693\n",
      "fidelity: 0 iter 28 nll:10.41202\n",
      "fidelity: 0 iter 29 nll:10.40712\n",
      "fidelity: 0 iter 30 nll:10.40222\n",
      "fidelity: 0 iter 31 nll:10.39733\n",
      "fidelity: 0 iter 32 nll:10.39244\n",
      "fidelity: 0 iter 33 nll:10.38755\n",
      "fidelity: 0 iter 34 nll:10.38268\n",
      "fidelity: 0 iter 35 nll:10.37780\n",
      "fidelity: 0 iter 36 nll:10.37293\n",
      "fidelity: 0 iter 37 nll:10.36806\n",
      "fidelity: 0 iter 38 nll:10.36320\n",
      "fidelity: 0 iter 39 nll:10.35835\n",
      "fidelity: 0 iter 40 nll:10.35350\n",
      "fidelity: 0 iter 41 nll:10.34865\n",
      "fidelity: 0 iter 42 nll:10.34381\n",
      "fidelity: 0 iter 43 nll:10.33898\n",
      "fidelity: 0 iter 44 nll:10.33415\n",
      "fidelity: 0 iter 45 nll:10.32932\n",
      "fidelity: 0 iter 46 nll:10.32450\n",
      "fidelity: 0 iter 47 nll:10.31968\n",
      "fidelity: 0 iter 48 nll:10.31487\n",
      "fidelity: 0 iter 49 nll:10.31007\n",
      "fidelity: 0 iter 50 nll:10.30527\n",
      "fidelity: 0 iter 51 nll:10.30048\n",
      "fidelity: 0 iter 52 nll:10.29569\n",
      "fidelity: 0 iter 53 nll:10.29091\n",
      "fidelity: 0 iter 54 nll:10.28613\n",
      "fidelity: 0 iter 55 nll:10.28136\n",
      "fidelity: 0 iter 56 nll:10.27659\n",
      "fidelity: 0 iter 57 nll:10.27184\n",
      "fidelity: 0 iter 58 nll:10.26708\n",
      "fidelity: 0 iter 59 nll:10.26233\n",
      "fidelity: 0 iter 60 nll:10.25759\n",
      "fidelity: 0 iter 61 nll:10.25285\n",
      "fidelity: 0 iter 62 nll:10.24812\n",
      "fidelity: 0 iter 63 nll:10.24340\n",
      "fidelity: 0 iter 64 nll:10.23868\n",
      "fidelity: 0 iter 65 nll:10.23397\n",
      "fidelity: 0 iter 66 nll:10.22926\n",
      "fidelity: 0 iter 67 nll:10.22456\n",
      "fidelity: 0 iter 68 nll:10.21987\n",
      "fidelity: 0 iter 69 nll:10.21518\n",
      "fidelity: 0 iter 70 nll:10.21050\n",
      "fidelity: 0 iter 71 nll:10.20583\n",
      "fidelity: 0 iter 72 nll:10.20116\n",
      "fidelity: 0 iter 73 nll:10.19650\n",
      "fidelity: 0 iter 74 nll:10.19184\n",
      "fidelity: 0 iter 75 nll:10.18719\n",
      "fidelity: 0 iter 76 nll:10.18255\n",
      "fidelity: 0 iter 77 nll:10.17791\n",
      "fidelity: 0 iter 78 nll:10.17328\n",
      "fidelity: 0 iter 79 nll:10.16866\n",
      "fidelity: 0 iter 80 nll:10.16405\n",
      "fidelity: 0 iter 81 nll:10.15944\n",
      "fidelity: 0 iter 82 nll:10.15484\n",
      "fidelity: 0 iter 83 nll:10.15024\n",
      "fidelity: 0 iter 84 nll:10.14566\n",
      "fidelity: 0 iter 85 nll:10.14108\n",
      "fidelity: 0 iter 86 nll:10.13650\n",
      "fidelity: 0 iter 87 nll:10.13194\n",
      "fidelity: 0 iter 88 nll:10.12738\n",
      "fidelity: 0 iter 89 nll:10.12283\n",
      "fidelity: 0 iter 90 nll:10.11828\n",
      "fidelity: 0 iter 91 nll:10.11375\n",
      "fidelity: 0 iter 92 nll:10.10922\n",
      "fidelity: 0 iter 93 nll:10.10469\n",
      "fidelity: 0 iter 94 nll:10.10018\n",
      "fidelity: 0 iter 95 nll:10.09567\n",
      "fidelity: 0 iter 96 nll:10.09118\n",
      "fidelity: 0 iter 97 nll:10.08668\n",
      "fidelity: 0 iter 98 nll:10.08220\n",
      "fidelity: 0 iter 99 nll:10.07772\n",
      "fidelity: 1 iter 0 nll:9.89048\n",
      "fidelity: 1 iter 1 nll:9.86051\n",
      "fidelity: 1 iter 2 nll:9.83084\n",
      "fidelity: 1 iter 3 nll:9.80146\n",
      "fidelity: 1 iter 4 nll:9.77237\n",
      "fidelity: 1 iter 5 nll:9.74358\n",
      "fidelity: 1 iter 6 nll:9.71508\n",
      "fidelity: 1 iter 7 nll:9.68689\n",
      "fidelity: 1 iter 8 nll:9.65899\n",
      "fidelity: 1 iter 9 nll:9.63140\n",
      "fidelity: 1 iter 10 nll:9.60411\n",
      "fidelity: 1 iter 11 nll:9.57712\n",
      "fidelity: 1 iter 12 nll:9.55043\n",
      "fidelity: 1 iter 13 nll:9.52404\n",
      "fidelity: 1 iter 14 nll:9.49795\n",
      "fidelity: 1 iter 15 nll:9.47216\n",
      "fidelity: 1 iter 16 nll:9.44667\n",
      "fidelity: 1 iter 17 nll:9.42147\n",
      "fidelity: 1 iter 18 nll:9.39658\n",
      "fidelity: 1 iter 19 nll:9.37197\n",
      "fidelity: 1 iter 20 nll:9.34767\n",
      "fidelity: 1 iter 21 nll:9.32365\n",
      "fidelity: 1 iter 22 nll:9.29992\n",
      "fidelity: 1 iter 23 nll:9.27647\n",
      "fidelity: 1 iter 24 nll:9.25331\n",
      "fidelity: 1 iter 25 nll:9.23044\n",
      "fidelity: 1 iter 26 nll:9.20784\n",
      "fidelity: 1 iter 27 nll:9.18553\n",
      "fidelity: 1 iter 28 nll:9.16348\n",
      "fidelity: 1 iter 29 nll:9.14171\n",
      "fidelity: 1 iter 30 nll:9.12021\n",
      "fidelity: 1 iter 31 nll:9.09898\n",
      "fidelity: 1 iter 32 nll:9.07800\n",
      "fidelity: 1 iter 33 nll:9.05729\n",
      "fidelity: 1 iter 34 nll:9.03684\n",
      "fidelity: 1 iter 35 nll:9.01663\n",
      "fidelity: 1 iter 36 nll:8.99668\n",
      "fidelity: 1 iter 37 nll:8.97698\n",
      "fidelity: 1 iter 38 nll:8.95752\n",
      "fidelity: 1 iter 39 nll:8.93830\n",
      "fidelity: 1 iter 40 nll:8.91932\n",
      "fidelity: 1 iter 41 nll:8.90057\n",
      "fidelity: 1 iter 42 nll:8.88206\n",
      "fidelity: 1 iter 43 nll:8.86376\n",
      "fidelity: 1 iter 44 nll:8.84570\n",
      "fidelity: 1 iter 45 nll:8.82786\n",
      "fidelity: 1 iter 46 nll:8.81023\n",
      "fidelity: 1 iter 47 nll:8.79282\n",
      "fidelity: 1 iter 48 nll:8.77562\n",
      "fidelity: 1 iter 49 nll:8.75863\n",
      "fidelity: 1 iter 50 nll:8.74184\n",
      "fidelity: 1 iter 51 nll:8.72526\n",
      "fidelity: 1 iter 52 nll:8.70887\n",
      "fidelity: 1 iter 53 nll:8.69268\n",
      "fidelity: 1 iter 54 nll:8.67668\n",
      "fidelity: 1 iter 55 nll:8.66088\n",
      "fidelity: 1 iter 56 nll:8.64526\n",
      "fidelity: 1 iter 57 nll:8.62982\n",
      "fidelity: 1 iter 58 nll:8.61456\n",
      "fidelity: 1 iter 59 nll:8.59949\n",
      "fidelity: 1 iter 60 nll:8.58459\n",
      "fidelity: 1 iter 61 nll:8.56986\n",
      "fidelity: 1 iter 62 nll:8.55530\n",
      "fidelity: 1 iter 63 nll:8.54091\n",
      "fidelity: 1 iter 64 nll:8.52668\n",
      "fidelity: 1 iter 65 nll:8.51262\n",
      "fidelity: 1 iter 66 nll:8.49871\n",
      "fidelity: 1 iter 67 nll:8.48497\n",
      "fidelity: 1 iter 68 nll:8.47138\n",
      "fidelity: 1 iter 69 nll:8.45794\n",
      "fidelity: 1 iter 70 nll:8.44465\n",
      "fidelity: 1 iter 71 nll:8.43151\n",
      "fidelity: 1 iter 72 nll:8.41851\n",
      "fidelity: 1 iter 73 nll:8.40566\n",
      "fidelity: 1 iter 74 nll:8.39295\n",
      "fidelity: 1 iter 75 nll:8.38038\n",
      "fidelity: 1 iter 76 nll:8.36794\n",
      "fidelity: 1 iter 77 nll:8.35564\n",
      "fidelity: 1 iter 78 nll:8.34348\n",
      "fidelity: 1 iter 79 nll:8.33144\n",
      "fidelity: 1 iter 80 nll:8.31954\n",
      "fidelity: 1 iter 81 nll:8.30776\n",
      "fidelity: 1 iter 82 nll:8.29610\n",
      "fidelity: 1 iter 83 nll:8.28457\n",
      "fidelity: 1 iter 84 nll:8.27316\n",
      "fidelity: 1 iter 85 nll:8.26187\n",
      "fidelity: 1 iter 86 nll:8.25070\n",
      "fidelity: 1 iter 87 nll:8.23965\n",
      "fidelity: 1 iter 88 nll:8.22870\n",
      "fidelity: 1 iter 89 nll:8.21788\n",
      "fidelity: 1 iter 90 nll:8.20716\n",
      "fidelity: 1 iter 91 nll:8.19656\n",
      "fidelity: 1 iter 92 nll:8.18606\n",
      "fidelity: 1 iter 93 nll:8.17567\n",
      "fidelity: 1 iter 94 nll:8.16538\n",
      "fidelity: 1 iter 95 nll:8.15520\n",
      "fidelity: 1 iter 96 nll:8.14512\n",
      "fidelity: 1 iter 97 nll:8.13514\n",
      "fidelity: 1 iter 98 nll:8.12526\n",
      "fidelity: 1 iter 99 nll:8.11548\n"
     ]
    }
   ],
   "source": [
    "fidelity_manager = MultiFidelityDataManager(initial_data)\n",
    "kernel1 = kernel.SquaredExponentialKernel(length_scale = 1., signal_variance = 1.)\n",
    "# model = AR(fidelity_num=2, kernel=kernel1, rho_init=1.0, if_nonsubset=True)\n",
    "# train_AR(model, fidelity_manager, max_iter=100, lr_init=1e-3)\n",
    "model = ResGP(fidelity_num=2, kernel=kernel1, if_nonsubset=True)\n",
    "train_ResGP(model, fidelity_manager, max_iter=100, lr_init=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the mean and variance functions for acq function\n",
    "The defined mean and variance functions extract the predictive mean and variance from the trained surrogate model (model) when provided with input points (x) and fidelity indicator (s). \n",
    "These functions are crucial components in the computation of acquisition functions, such as the Upper Confidence Bound (UCB), and are used to guide the selection of the next point for evaluation in the Bayesian optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_function(x, s):\n",
    "    mean, _ = model.forward(fidelity_manager, x, s)\n",
    "    return mean.reshape(-1, 1)\n",
    "    \n",
    "def variance_function(x, s):\n",
    "    _, variance = model.forward(fidelity_manager, x, s)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and optimize MF_acq function\n",
    "\n",
    "In this section, we initiate a sample for MF discrete acquistion function. According to the different acq, there is a little different between the initiate parameter setting.\n",
    "If want to use the KG/EI/PI acq, need specify the current best function (f_best) when initiate. But UCB do not need the f_best when initiating.\n",
    "\n",
    "Then we use the Opitmizer to find the new_x and use different selection strategy to select next_s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 x: Parameter containing:\n",
      "tensor([[0.4920]], requires_grad=True) Negative Acquisition Function: -0.1446552574634552\n",
      "iter 1 x: Parameter containing:\n",
      "tensor([[0.4824]], requires_grad=True) Negative Acquisition Function: -0.14496617019176483\n",
      "iter 2 x: Parameter containing:\n",
      "tensor([[0.4728]], requires_grad=True) Negative Acquisition Function: -0.14527581632137299\n",
      "iter 3 x: Parameter containing:\n",
      "tensor([[0.4632]], requires_grad=True) Negative Acquisition Function: -0.1455923616886139\n",
      "iter 4 x: Parameter containing:\n",
      "tensor([[0.4536]], requires_grad=True) Negative Acquisition Function: -0.1459185779094696\n",
      "iter 5 x: Parameter containing:\n",
      "tensor([[0.4439]], requires_grad=True) Negative Acquisition Function: -0.1462557166814804\n",
      "iter 6 x: Parameter containing:\n",
      "tensor([[0.4341]], requires_grad=True) Negative Acquisition Function: -0.14660455286502838\n",
      "iter 7 x: Parameter containing:\n",
      "tensor([[0.4241]], requires_grad=True) Negative Acquisition Function: -0.14696544408798218\n",
      "iter 8 x: Parameter containing:\n",
      "tensor([[0.4141]], requires_grad=True) Negative Acquisition Function: -0.1473386585712433\n",
      "iter 9 x: Parameter containing:\n",
      "tensor([[0.4039]], requires_grad=True) Negative Acquisition Function: -0.14772433042526245\n",
      "iter 0 x: Parameter containing:\n",
      "tensor([[0.0828]], requires_grad=True) Negative Acquisition Function: -0.35769951343536377\n",
      "iter 1 x: Parameter containing:\n",
      "tensor([[0.0732]], requires_grad=True) Negative Acquisition Function: -0.35816091299057007\n",
      "iter 2 x: Parameter containing:\n",
      "tensor([[0.0636]], requires_grad=True) Negative Acquisition Function: -0.35860493779182434\n",
      "iter 3 x: Parameter containing:\n",
      "tensor([[0.0540]], requires_grad=True) Negative Acquisition Function: -0.3590441644191742\n",
      "iter 4 x: Parameter containing:\n",
      "tensor([[0.0443]], requires_grad=True) Negative Acquisition Function: -0.35948291420936584\n",
      "iter 5 x: Parameter containing:\n",
      "tensor([[0.0346]], requires_grad=True) Negative Acquisition Function: -0.35992300510406494\n",
      "iter 6 x: Parameter containing:\n",
      "tensor([[0.0247]], requires_grad=True) Negative Acquisition Function: -0.3603653907775879\n",
      "iter 7 x: Parameter containing:\n",
      "tensor([[0.0147]], requires_grad=True) Negative Acquisition Function: -0.36081045866012573\n",
      "iter 8 x: Parameter containing:\n",
      "tensor([[0.0046]], requires_grad=True) Negative Acquisition Function: -0.361258327960968\n",
      "iter 9 x: Parameter containing:\n",
      "tensor([[-0.0056]], requires_grad=True) Negative Acquisition Function: -0.3617091178894043\n",
      "tensor([[-0.0056]]) 2\n"
     ]
    }
   ],
   "source": [
    "acq = DMF_UCB(mean_function, variance_function, 2, train_xl.shape[1], torch.ones(1).reshape(-1, 1))\n",
    "\n",
    "# Use Opitmizer to find the new_x\n",
    "new_x = optimize_acq_mf(fidelity_manager, acq, 10, 0.01) \n",
    "# Use different selection strategy to select next_s\n",
    "new_s = acq.acq_selection_fidelity(gamma=[0.1, 0.1], new_x=new_x)\n",
    "print(new_x, new_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
