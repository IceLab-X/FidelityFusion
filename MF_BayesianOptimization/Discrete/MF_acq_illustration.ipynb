{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yuxinwang/Desktop/FidelityFusion/MF_BayesianOptimization\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "os.chdir(os.path.dirname(os.path.realpath(\".\")))\n",
    "sys.path.insert(0, '..')\n",
    "print(os.path.realpath(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF Acquisition Function Illustration file\n",
    "在这个notebook中，我将讲述几种多精度的贝叶斯优化中采集函数的数学方法以及代码实现，分别是MF_UCB，MF_EI， Fabolas以及cfKG。\n",
    "<!-- ![jupyter](./assets/figures/Summary_of_MFBO_lit.png) -->\n",
    "<!-- <img src=\"./assets/figures/Summary_of_MFBO_lit.png\", width=320, heigth=240> -->\n",
    "对于采集函数来说，我们尽可能地简化他们的依赖，努力将输入控制为后验分布的均值函数，后验分布的方差函数 以及 反应采集不同精度数据的成本差异的cost函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF_UCB\n",
    "Based on paper \"Gaussian Process Bandit Optimisation with Multi-fidelity Evaluations\" by Kirthevasan Kandasamy, Gautam Dasarathy (link: https://proceedings.neurips.cc/paper_files/paper/2016/file/605ff764c617d3cd28dbbdd72be8f9a2-Paper.pdf)\n",
    "\n",
    "In MF_UCB, the method are divided into two parts: \n",
    "<!-- 1. 用高低精度分别做UCB，记为UCB_l&UCB_h。min(UCB_l, UCB_h)作为MF-UCB，选择最大值；\n",
    "2. 人为设置精度判断指标，其中beta_t和gamma为作者设定的，sigma是x_t点的方差。如果满足下式选低精度，ortherwise选高精度 -->\n",
    "\n",
    "1. Implement UCB using low precision and high precision separately, denoted as UCB_l and UCB_h. Consider the minimum of UCB_l and UCB_h as MF-UCB, and choose the maximum value.\n",
    "    <!-- $$ \\phi_t^{(m)}(x) = \\mu_{t-1}^{(m)}(x) + \\beta_t^{1/2}\\sigma_{t-1}^{(m)}(x) $$ -->\n",
    "\n",
    "    \\begin{aligned}\n",
    "    \\phi_t^{(m)}(x) &= \\mu_{t-1}^{(m)}(x) + \\beta_t^{1/2}\\sigma_{t-1}^{(m)}(x), \\\\\n",
    "    \\phi_t(x) &= \\min_m \\phi_t^{(m)}(x), \\\\\n",
    "    x_t &= \\argmax_{x\\in\\mathcal{X}}\\phi_t^{(m)}(x), \\\\\n",
    "    \\end{aligned}\n",
    "    where $m$ denotes the $m^{th}$ fidelity, $\\beta_t$ coefficient trading off exploration and exploitation in $t$-th interation, $\\phi_t^{(m)}(x)$ denotes the upper confidence bound (UCB) and $\\phi_t(x)$ denotes the combined UCB provided by all fidelities.\n",
    "    \n",
    "    Choice of $\\beta_t$, the paper use $\\beta_t = 0.2 d \\log(2t)$ where $t$ means $t$-th interation and $d$ means the dimension of $x$.\n",
    "\n",
    "2. Establish precision evaluation criteria based on manually set indicators, where $\\beta_t$ and $\\gamma$ are predefined by the author, and sigma represents the variance at point $x_t$. If the following condition is met, $\\gamma^{(m)}$ is the parameter in MF_UCB for switching from the $m$th fidelity to the $(m+1)$th fidelity. This parameter is setting by author.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB_compute_x(x, mean_function, var_function, BO_iteration_count, fidelity_num):    \n",
    "    N_UCB = []\n",
    "    UCB_x = []\n",
    "    beta = 0.2 * int(x.shape[0]) * torch.log(BO_iteration_count)\n",
    "    for i in range(fidelity_num):\n",
    "        torch.manual_seed(2024)\n",
    "        x = nn.Parameter(torch.from_numpy(torch.rand(1, x.shape[0])).double())\n",
    "        # optimise_adam(fidelity_indicator=i+1, niteration=15, lr=0.01)\n",
    "        UCB_x.append(x.detach())\n",
    "        mean = mean_function(x, i+1)\n",
    "        var = var_function(x, i+1)\n",
    "        UCB = mean + beta * var\n",
    "        N_UCB.append(UCB)\n",
    "\n",
    "    new_x = UCB_x[N_UCB.index(min(N_UCB))]\n",
    "\n",
    "    return new_x\n",
    "\n",
    "def UCB_compute_s(x, mean_function, var_function, fidelity_num):\n",
    "    beta = 0.2 * int(x.shape[0])\n",
    "    gamma = 0.1\n",
    "    fidelity_list = []\n",
    "    for i in range(fidelity_num):\n",
    "        mean = mean_function(x, i+1)\n",
    "        var = var_function(x, i+1)\n",
    "        UCB = mean + beta * var\n",
    "        if UCB > gamma:\n",
    "            fidelity_list.append(i)\n",
    "\n",
    "    new_s = min(fidelity_list)\n",
    "\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF_EI\n",
    "Based on paper \"Gaussian Process Bandit Optimisation with Multi-fidelity Evaluations\" by Kirthevasan Kandasamy, Gautam Dasarathy (link: https://proceedings.neurips.cc/paper_files/paper/2016/file/605ff764c617d3cd28dbbdd72be8f9a2-Paper.pdf)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
