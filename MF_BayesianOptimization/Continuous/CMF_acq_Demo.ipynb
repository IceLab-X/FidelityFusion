{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "# print(os.path.dirname(os.path.realpath(\".\")))\n",
    "os.chdir(os.path.dirname(os.path.realpath(\".\")))\n",
    "\n",
    "import GaussianProcess.kernel as kernel\n",
    "from FidelityFusion_Models import *\n",
    "from FidelityFusion_Models.MF_data import MultiFidelityDataManager\n",
    "from CMF_acq import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Simple Two fidelity objective function\n",
    "This is a very simple two fidelity sample, named as 'Non Linear Sin' function.\n",
    "\\begin{aligned}\n",
    "    f_{low}(x) &= \\sin(8\\pi x),\\\\\n",
    "    f_{high}(x) &= (x - \\sqrt{2})f_{low}(x)^2.\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "We use this objective function to generate a small size training data set, with 8 low-fidleity samples and 4 high-fidelity training samples under the non-subset assumption.\n",
    "\n",
    "To be mentioned, the fidelity indicator starts from 0, which means 0 represents low-fidleity and 1 stands for high-fidleity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x, s):\n",
    "    xtr = x\n",
    "    if s == 0:\n",
    "        Ytr = torch.sin(xtr * 8 * torch.pi)\n",
    "    else:\n",
    "        Ytr_l = torch.sin(xtr * 8 * torch.pi)\n",
    "        Ytr = (xtr - torch.sqrt(torch.ones(xtr.shape[0])*2).reshape(-1, 1)) * torch.pow(Ytr_l, 2)\n",
    "    \n",
    "    return Ytr\n",
    "\n",
    "train_xl = torch.rand(8, 1) * 10\n",
    "train_xh = torch.rand(4, 1) * 10\n",
    "train_yl = objective_function(train_xl, 0)\n",
    "train_yh = objective_function(train_xh, 1)\n",
    "\n",
    "data_shape = [train_yl[0].shape, train_yh[0].shape]\n",
    "\n",
    "initial_data = [\n",
    "                    {'fidelity_indicator': 0,'raw_fidelity_name': '0', 'X': train_xl, 'Y': train_yl},\n",
    "                    {'fidelity_indicator': 1, 'raw_fidelity_name': '1','X': train_xh, 'Y': train_yh},\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the surrogate model: ResGP\n",
    "\n",
    "These part we use the simplest surrogate model ResGP as our MF model and train by the initiated training dataset. It can also be replaced by other MF model in FidelityFusion Models (i.e. AR, GAR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fidelity 0, epoch 100/100, nll: 13.202631950378418\n",
      "fidelity 1, epoch 100/100, nll: 6.927824974060059\n"
     ]
    }
   ],
   "source": [
    "fidelity_manager = MultiFidelityDataManager(initial_data)\n",
    "kernel1 = kernel.SquaredExponentialKernel(length_scale = 1., signal_variance = 1.)\n",
    "# model = AR(fidelity_num=2, kernel=kernel1, rho_init=1.0, if_nonsubset=True)\n",
    "# train_AR(model, fidelity_manager, max_iter=100, lr_init=1e-3)\n",
    "fidelity_num = 2\n",
    "kernel_list = [kernel.SquaredExponentialKernel() for _ in range(fidelity_num)]\n",
    "# myResGP = ResGP(fidelity_num = 3,kernel_list=kernel_list, if_nonsubset = True).to(device)\n",
    "model = ResGP(fidelity_num=2, kernel_list=kernel_list, if_nonsubset=True)\n",
    "train_ResGP(model, fidelity_manager, max_iter=100, lr_init=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the mean and variance functions for acq function\n",
    "The defined mean and variance functions extract the predictive mean and variance from the trained surrogate model (model) when provided with input points (x) and fidelity indicator (s). \n",
    "These functions are crucial components in the computation of acquisition functions, such as the Upper Confidence Bound (UCB), and are used to guide the selection of the next point for evaluation in the Bayesian optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_function(x, s):\n",
    "    mean, _ = model.forward(fidelity_manager, x, s)\n",
    "    return mean.reshape(-1, 1)\n",
    "    \n",
    "def variance_function(x, s):\n",
    "    _, variance = model.forward(fidelity_manager, x, s)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and optimize MF_acq function\n",
    "\n",
    "In this section, we initiate a sample for MF discrete acquistion function. According to the different acq, there is a little different between the initiate parameter setting.\n",
    "If want to use the KG/EI/PI acq, need specify the current best function (f_best) when initiate. But UCB do not need the f_best when initiating.\n",
    "\n",
    "Then we use the Opitmizer to find the new_x and use different selection strategy to select next_s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 x: Parameter containing:\n",
      "tensor([[0.4920]], requires_grad=True) Negative Acquisition Function: -0.1446552574634552\n",
      "iter 1 x: Parameter containing:\n",
      "tensor([[0.4824]], requires_grad=True) Negative Acquisition Function: -0.14496617019176483\n",
      "iter 2 x: Parameter containing:\n",
      "tensor([[0.4728]], requires_grad=True) Negative Acquisition Function: -0.14527581632137299\n",
      "iter 3 x: Parameter containing:\n",
      "tensor([[0.4632]], requires_grad=True) Negative Acquisition Function: -0.1455923616886139\n",
      "iter 4 x: Parameter containing:\n",
      "tensor([[0.4536]], requires_grad=True) Negative Acquisition Function: -0.1459185779094696\n",
      "iter 5 x: Parameter containing:\n",
      "tensor([[0.4439]], requires_grad=True) Negative Acquisition Function: -0.1462557166814804\n",
      "iter 6 x: Parameter containing:\n",
      "tensor([[0.4341]], requires_grad=True) Negative Acquisition Function: -0.14660455286502838\n",
      "iter 7 x: Parameter containing:\n",
      "tensor([[0.4241]], requires_grad=True) Negative Acquisition Function: -0.14696544408798218\n",
      "iter 8 x: Parameter containing:\n",
      "tensor([[0.4141]], requires_grad=True) Negative Acquisition Function: -0.1473386585712433\n",
      "iter 9 x: Parameter containing:\n",
      "tensor([[0.4039]], requires_grad=True) Negative Acquisition Function: -0.14772433042526245\n",
      "iter 0 x: Parameter containing:\n",
      "tensor([[0.0828]], requires_grad=True) Negative Acquisition Function: -0.35769951343536377\n",
      "iter 1 x: Parameter containing:\n",
      "tensor([[0.0732]], requires_grad=True) Negative Acquisition Function: -0.35816091299057007\n",
      "iter 2 x: Parameter containing:\n",
      "tensor([[0.0636]], requires_grad=True) Negative Acquisition Function: -0.35860493779182434\n",
      "iter 3 x: Parameter containing:\n",
      "tensor([[0.0540]], requires_grad=True) Negative Acquisition Function: -0.3590441644191742\n",
      "iter 4 x: Parameter containing:\n",
      "tensor([[0.0443]], requires_grad=True) Negative Acquisition Function: -0.35948291420936584\n",
      "iter 5 x: Parameter containing:\n",
      "tensor([[0.0346]], requires_grad=True) Negative Acquisition Function: -0.35992300510406494\n",
      "iter 6 x: Parameter containing:\n",
      "tensor([[0.0247]], requires_grad=True) Negative Acquisition Function: -0.3603653907775879\n",
      "iter 7 x: Parameter containing:\n",
      "tensor([[0.0147]], requires_grad=True) Negative Acquisition Function: -0.36081045866012573\n",
      "iter 8 x: Parameter containing:\n",
      "tensor([[0.0046]], requires_grad=True) Negative Acquisition Function: -0.361258327960968\n",
      "iter 9 x: Parameter containing:\n",
      "tensor([[-0.0056]], requires_grad=True) Negative Acquisition Function: -0.3617091178894043\n",
      "tensor([[-0.0056]]) 2\n"
     ]
    }
   ],
   "source": [
    "acq = CMF_UCB(mean_function, variance_function, 2, train_xl.shape[1], torch.ones(1).reshape(-1, 1))\n",
    "\n",
    "# Use Opitmizer to find the new_x\n",
    "new_x = optimize_acq_mf(fidelity_manager, acq, 10, 0.01) \n",
    "# Use different selection strategy to select next_s\n",
    "new_s = acq.acq_selection_fidelity(gamma=[0.1, 0.1], new_x=new_x)\n",
    "print(new_x, new_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
