{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from acq import UCB, EI, PI, KG, find_next_batch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "sys.path.append(\"../GaussianProcess\")\n",
    "import GaussianProcess.kernel as kernel\n",
    "from cigp import CIGP_withMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function\n",
    "This objective function represents a simple sum of sine functions. The goal is to demonstrate a basic mathematical function that exhibits periodic behavior with different frequencies. The input 'x' is the variable at which the sine functions are evaluated, and the result is the sum of sin(x) and sin(2x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x):\n",
    "    # Simple sum of sine functions for demonstration\n",
    "    return torch.sin(x)+torch.sin(2*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize prior knowledge with 5 random points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "num_initial_points = 5\n",
    "train_x = torch.rand(num_initial_points, input_dim) * 10  # Random points in [0, 10] for each dimension\n",
    "train_y = objective_function(train_x).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the surrogate model\n",
    "This code initializes a surrogate model for Bayesian optimization. The surrogate model (CIGP_withMean) is equipped with a chosen kernel (ARDKernel) to capture the underlying patterns in the data.  The Adam optimizer is then set up to optimize the model's parameters during the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel1 = kernel.ARDKernel(1)\n",
    "# kernel1 = kernel.MaternKernel(1)\n",
    "# kernel1 = kernel.LinearKernel(1,-1.0,1.)\n",
    "# kernel1 = kernel.SumKernel(kernel.LinearKernel(1), kernel.MaternKernel(1))\n",
    "model = CIGP_withMean(1, 1, kernel=kernel1, noise_variance=2.)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the mean and variance functions for acq function\n",
    "The defined mean and variance functions extract the predictive mean and variance from the trained surrogate model (model) when provided with input points (X). These functions are crucial components in the computation of acquisition functions, such as the Upper Confidence Bound (UCB), and are used to guide the selection of the next point for evaluation in the Bayesian optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_function(X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mean, _ = model.forward(train_x, train_y, X)\n",
    "        return mean\n",
    "\n",
    "def variance_function(X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, var = model.forward(train_x, train_y, X)\n",
    "        return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize acq function\n",
    "The code snippet initializes different acquisition functions for Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb = UCB(mean_function, variance_function, kappa=5)\n",
    "pi = PI(mean_function, variance_function)\n",
    "ei = EI(mean_function, variance_function)\n",
    "kg = KG(mean_function, variance_function, num_fantasies=10)\n",
    "best_y = []\n",
    "# use it to remember the key iteration\n",
    "key_iterations = [2,4,5,6,8,10]\n",
    "predictions = []\n",
    "iteration_label = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization loop\n",
    "The key iterations allow monitoring the model's behavior and predictions at specific points during the optimization process. The overall process aims to iteratively improve the surrogate model and select points for evaluation that are expected to yield the best objective values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimization loop\n",
    "bounds = np.array([[0, 10]] * input_dim)\n",
    "for iteration in range(10):  # Run for 5 iterations\n",
    "\n",
    "    for i in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        loss = -model.log_likelihood(train_x, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('iter', i, 'nll:{:.5f}'.format(loss.item()))\n",
    "\n",
    "\n",
    "    batch_points = find_next_batch(ucb, bounds, batch_size=1, n_samples=500, f_best=train_x[np.argmax(train_y)])\n",
    "    # batch_points = ei.find_next_batch(bounds, batch_size=1, n_samples=1000, f_best=train_x[np.argmax(train_y)])\n",
    "    #find_next_batch(acq)\n",
    "    batch_points = torch.tensor(batch_points).float()\n",
    "\n",
    "    # Evaluate the objective function\n",
    "    new_y = objective_function(batch_points.squeeze()).reshape(-1,1)\n",
    "\n",
    "    # Update the model\n",
    "    train_x = torch.cat([train_x, batch_points])\n",
    "    train_y = torch.cat([train_y, new_y])\n",
    "    # Store the best objective value found so far\n",
    "    best_y.append(new_y.max().item())\n",
    "    # Visualization\n",
    "\n",
    "    # 在关键迭代时保存模型预测\n",
    "    if (iteration + 1) in key_iterations:\n",
    "        model.eval()\n",
    "        fixed_dims = torch.full((1, input_dim - 1), 5.0)  # Example: set them to the midpoint (5.0)\n",
    "        test_points = torch.linspace(0, 10, 100)\n",
    "        test_X = torch.cat((test_points.unsqueeze(1), fixed_dims.expand(test_points.size(0), -1)), 1)\n",
    "        true_y = objective_function(test_X)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_mean, pred_std = model.forward(train_x, train_y, test_X)\n",
    "            predictions.append((pred_mean, pred_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the picture\n",
    "This visualization helps to understand how well the Gaussian Process model captures the true function and how uncertainty evolves as more samples are acquired during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制子图\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, (pred_mean, pred_std) in enumerate(predictions):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.plot(test_points.numpy(), true_y.numpy(), 'k-', label='True function')\n",
    "    plt.plot(test_points.numpy(), pred_mean.numpy(), 'b--', label='GP mean')\n",
    "    plt.fill_between(test_points.numpy().reshape(-1),\n",
    "                     (pred_mean - 2 * pred_std).numpy().reshape(-1),\n",
    "                     (pred_mean + 2 * pred_std).numpy().reshape(-1),\n",
    "                     color='blue', alpha=0.2, label='GP uncertainty')\n",
    "\n",
    "    observed_x = train_x[:, 0].numpy()  # Only the first dimension for all observed points\n",
    "    observed_y = train_y.numpy()\n",
    "    plt.scatter(observed_x[:num_initial_points+key_iterations[i]], observed_y[:num_initial_points+key_iterations[i]], c='r', zorder=3, label='Observed points')\n",
    "    plt.title(f'Samples: {key_iterations[i]}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
