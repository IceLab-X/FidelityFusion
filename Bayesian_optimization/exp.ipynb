{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'BO.BO'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minimize\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBO\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBO\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAcquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UCB, EI, PI, KG, find_next_batch\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'BO.BO'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\24779\\\\Downloads\\\\BO\\\\BO')\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from BO.BO.Acquisition.acq import UCB, EI, PI, KG, find_next_batch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kernel as kernel\n",
    "from cigp_withMean import CIGP_withMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function\n",
    "This objective function represents a simple sum of sine functions. The goal is to demonstrate a basic mathematical function that exhibits periodic behavior with different frequencies. The input 'x' is the variable at which the sine functions are evaluated, and the result is the sum of sin(x) and sin(2x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x):\n",
    "    # Simple sum of sine functions for demonstration\n",
    "    return torch.sin(x)+torch.sin(2*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize prior knowledge with 5 random points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "num_initial_points = 5\n",
    "train_x = torch.rand(num_initial_points, input_dim) * 10  # Random points in [0, 10] for each dimension\n",
    "train_y = objective_function(train_x).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the surrogate model\n",
    "This code initializes a surrogate model for Bayesian optimization. The surrogate model (CIGP_withMean) is equipped with a chosen kernel (ARDKernel) to capture the underlying patterns in the data.  The Adam optimizer is then set up to optimize the model's parameters during the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel1 = kernel.ARDKernel(1)\n",
    "# kernel1 = kernel.MaternKernel(1)\n",
    "# kernel1 = kernel.LinearKernel(1,-1.0,1.)\n",
    "# kernel1 = kernel.SumKernel(kernel.LinearKernel(1), kernel.MaternKernel(1))\n",
    "model = CIGP_withMean(1, 1, kernel=kernel1, noise_variance=2.)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the mean and variance functions for acq function\n",
    "The defined mean and variance functions extract the predictive mean and variance from the trained surrogate model (model) when provided with input points (X). These functions are crucial components in the computation of acquisition functions, such as the Upper Confidence Bound (UCB), and are used to guide the selection of the next point for evaluation in the Bayesian optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_function(X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mean, _ = model.forward(train_x, train_y, X)\n",
    "        return mean\n",
    "\n",
    "def variance_function(X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, var = model.forward(train_x, train_y, X)\n",
    "        return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize acq function\n",
    "The code snippet initializes different acquisition functions for Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb = UCB(mean_function, variance_function, kappa=5)\n",
    "pi = PI(mean_function, variance_function)\n",
    "ei = EI(mean_function, variance_function)\n",
    "kg = KG(mean_function, variance_function, num_fantasies=10)\n",
    "best_y = []\n",
    "# use it to remember the key iteration\n",
    "key_iterations = [2,4,5,6,8,10]\n",
    "predictions = []\n",
    "iteration_label = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = np.array([[0, 10]] * input_dim)\n",
    "for iteration in range(10):  # Run for 5 iterations\n",
    "\n",
    "    for i in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        loss = -model.log_likelihood(train_x, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('iter', i, 'nll:{:.5f}'.format(loss.item()))\n",
    "\n",
    "    batch_points = find_next_batch(ucb, bounds, batch_size=1, n_samples=500, f_best=train_x[np.argmax(train_y)])\n",
    "    \n",
    "    batch_points = torch.tensor(batch_points).float()\n",
    "\n",
    "    # Evaluate the objective function\n",
    "    new_y = objective_function(batch_points.squeeze()).reshape(-1,1)\n",
    "\n",
    "    # Update the model\n",
    "    train_x = torch.cat([train_x, batch_points])\n",
    "    train_y = torch.cat([train_y, new_y])\n",
    "    # Store the best objective value found so far\n",
    "    best_y.append(new_y.max().item())\n",
    "    # Visualization\n",
    "\n",
    "    # 在关键迭代时保存模型预测\n",
    "    if (iteration + 1) in key_iterations:\n",
    "        model.eval()\n",
    "        fixed_dims = torch.full((1, input_dim - 1), 5.0)  # Example: set them to the midpoint (5.0)\n",
    "        test_points = torch.linspace(0, 10, 100)\n",
    "        test_X = torch.cat((test_points.unsqueeze(1), fixed_dims.expand(test_points.size(0), -1)), 1)\n",
    "        true_y = objective_function(test_X)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_mean, pred_std = model.forward(train_x, train_y, test_X)\n",
    "            predictions.append((pred_mean, pred_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "for i, (pred_mean, pred_std) in enumerate(predictions):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.plot(test_points.numpy(), true_y.numpy(), 'k-', label='True function')\n",
    "    plt.plot(test_points.numpy(), pred_mean.numpy(), 'b--', label='GP mean')\n",
    "    plt.fill_between(test_points.numpy().reshape(-1),\n",
    "                     (pred_mean - 2 * pred_std).numpy().reshape(-1),\n",
    "                     (pred_mean + 2 * pred_std).numpy().reshape(-1),\n",
    "                     color='blue', alpha=0.2, label='GP uncertainty')\n",
    "\n",
    "    observed_x = train_x[:, 0].numpy()  # Only the first dimension for all observed points\n",
    "    observed_y = train_y.numpy()\n",
    "    plt.scatter(observed_x[:num_initial_points+key_iterations[i]], observed_y[:num_initial_points+key_iterations[i]], c='r', zorder=3, label='Observed points')\n",
    "    plt.title(f'Samples: {key_iterations[i]}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
